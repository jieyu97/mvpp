{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Jupyter notebook is for DRN univariate post-processing on temperature forecasts,\n",
    "# using the data from https://doi.org/10.6084/m9.figshare.19453580\n",
    "# The script is an adapted version of the replication Jupyter notebook from https://github.com/slerch/ppnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "\n",
    "data = pd.read_feather(path = \"./temperature_data_cgm_std.feather\")\n",
    "data = data.iloc[:, list(range(0,3))+list(range(53,91))]\n",
    "data.station = pd.to_numeric(data.station, downcast = 'integer')\n",
    "\n",
    "# split into train and test data\n",
    "eval_start = 1626724\n",
    "train_end = 1626724\n",
    "\n",
    "train_features_raw = data.iloc[:train_end,3:].to_numpy()\n",
    "train_targets = data.iloc[:train_end,2].to_numpy()\n",
    "train_IDs = data.iloc[:train_end,1].to_numpy()\n",
    "\n",
    "test_features_raw = data.iloc[eval_start:,3:].to_numpy()\n",
    "test_targets = data.iloc[eval_start:,2].to_numpy()\n",
    "test_IDs = data.iloc[eval_start:,1].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data\n",
    "\n",
    "def normalize(data, method=None, shift=None, scale=None):\n",
    "    result = np.zeros(data.shape)\n",
    "    if method == \"MAX\":\n",
    "        scale = np.max(data, axis=0)\n",
    "        shift = np.zeros(scale.shape)\n",
    "    for index in range(len(data[0])):\n",
    "        result[:,index] = (data[:,index] - shift[index]) / scale[index]\n",
    "    return result, shift, scale\n",
    "\n",
    "train_features, train_shift, train_scale = normalize(train_features_raw[:,:], method=\"MAX\")\n",
    "\n",
    "test_features = normalize(test_features_raw[:,:], shift=train_shift, scale=train_scale)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions for NN models\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers import Input, Dense, merge, Embedding, Flatten, Concatenate\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "def crps_cost_function(y_true, y_pred, theano=False):\n",
    "    \"\"\"Compute the CRPS cost function for a normal distribution defined by\n",
    "    the mean and standard deviation.\n",
    "\n",
    "    Code inspired by Kai Polsterer (HITS).\n",
    "\n",
    "    Args:\n",
    "        y_true: True values\n",
    "        y_pred: Tensor containing predictions: [mean, std]\n",
    "        theano: Set to true if using this with pure theano.\n",
    "\n",
    "    Returns:\n",
    "        mean_crps: Scalar with mean CRPS over batch\n",
    "    \"\"\"\n",
    "\n",
    "    # Split input\n",
    "    mu = y_pred[:, 0]\n",
    "    sigma = y_pred[:, 1]\n",
    "    # Ugly workaround for different tensor allocation in keras and theano\n",
    "    if not theano:\n",
    "        y_true = y_true[:, 0]   # Need to also get rid of axis 1 to match!\n",
    "\n",
    "    # To stop sigma from becoming negative we first have to \n",
    "    # convert it the the variance and then take the square\n",
    "    # root again. \n",
    "    var = K.square(sigma)\n",
    "    # The following three variables are just for convenience\n",
    "    loc = (y_true - mu) / K.sqrt(var)\n",
    "    phi = 1.0 / np.sqrt(2.0 * np.pi) * K.exp(-K.square(loc) / 2.0)\n",
    "    Phi = 0.5 * (1.0 + tf.math.erf(loc / np.sqrt(2.0)))\n",
    "    # First we will compute the crps for each input/target pair\n",
    "    crps =  K.sqrt(var) * (loc * (2. * Phi - 1.) + 2 * phi - 1. / np.sqrt(np.pi))\n",
    "    # Then we take the mean. The cost is now a scalar\n",
    "    return K.mean(crps)\n",
    "\n",
    "def build_emb_model(n_features, n_outputs, hidden_nodes, emb_size, max_id,\n",
    "                    compile=False, optimizer='adam', lr=0.01,\n",
    "                    loss=crps_cost_function,\n",
    "                    activation='relu', reg=None):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        n_features: Number of features\n",
    "        n_outputs: Number of outputs\n",
    "        hidden_nodes: int or list of hidden nodes\n",
    "        emb_size: Embedding size\n",
    "        max_id: Max embedding ID\n",
    "        compile: If true, compile model\n",
    "        optimizer: Name of optimizer\n",
    "        lr: learning rate\n",
    "        loss: loss function\n",
    "        activation: Activation function for hidden layer\n",
    "\n",
    "    Returns:\n",
    "        model: Keras model\n",
    "    \"\"\"\n",
    "    if type(hidden_nodes) is not list:\n",
    "        hidden_nodes = [hidden_nodes]\n",
    "\n",
    "    features_in = Input(shape=(n_features,))\n",
    "    id_in = Input(shape=(1,))\n",
    "    emb = Embedding(max_id + 1, emb_size)(id_in)\n",
    "    emb = Flatten()(emb)\n",
    "    x = Concatenate()([features_in, emb])\n",
    "    for h in hidden_nodes:\n",
    "        x = Dense(h, activation=activation, kernel_regularizer=reg)(x)\n",
    "    x = Dense(n_outputs, activation='linear', kernel_regularizer=reg)(x)\n",
    "    model = Model(inputs=[features_in, id_in], outputs=x)\n",
    "\n",
    "    if compile:\n",
    "        opt = keras.optimizers.Adam(lr=lr)\n",
    "        model.compile(optimizer=opt, loss=loss)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b834f7e214f84c89bcddff96af6603e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gm2154\\Anaconda3\\envs\\python-mlpp\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\gm2154\\Anaconda3\\envs\\python-mlpp\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\gm2154\\Anaconda3\\envs\\python-mlpp\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\gm2154\\Anaconda3\\envs\\python-mlpp\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\gm2154\\Anaconda3\\envs\\python-mlpp\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\gm2154\\Anaconda3\\envs\\python-mlpp\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\gm2154\\Anaconda3\\envs\\python-mlpp\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\gm2154\\Anaconda3\\envs\\python-mlpp\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\gm2154\\Anaconda3\\envs\\python-mlpp\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\gm2154\\Anaconda3\\envs\\python-mlpp\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# training multiple models in a loop\n",
    "\n",
    "emb_size = 2\n",
    "max_id = int(np.max([train_IDs.max(), test_IDs.max()]))\n",
    "n_features = train_features.shape[1]\n",
    "n_outputs = 2\n",
    "\n",
    "nreps = 10\n",
    "trn_scores = []\n",
    "test_scores = []\n",
    "preds = []\n",
    "repred = []\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "for i in tqdm(range(nreps)):\n",
    "    \n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    keras.backend.clear_session()\n",
    "    \n",
    "    features_in = Input(shape=(n_features,))\n",
    "    id_in = Input(shape=(1,))\n",
    "    emb = Embedding(max_id + 1, emb_size)(id_in)\n",
    "    emb = Flatten()(emb)\n",
    "    x = Concatenate()([features_in, emb])\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dense(n_outputs, activation='linear')(x)\n",
    "    nn_aux_emb = Model(inputs=[features_in, id_in], outputs=x)\n",
    "\n",
    "    opt = keras.optimizers.Adam(lr=0.002)\n",
    "    nn_aux_emb.compile(optimizer=opt, loss=crps_cost_function)\n",
    "    \n",
    "    nn_aux_emb.fit([train_features, train_IDs], train_targets, epochs=15, batch_size=4096, verbose=0)   \n",
    "    \n",
    "    trn_scores.append(nn_aux_emb.evaluate([train_features, train_IDs], train_targets, 4096, verbose=0))\n",
    "    test_scores.append(nn_aux_emb.evaluate([test_features, test_IDs], test_targets, 4096, verbose=0))\n",
    "    \n",
    "    preds.append(nn_aux_emb.predict([test_features, test_IDs], 4096, verbose=0))\n",
    "    repred.append(nn_aux_emb.predict([train_features, train_IDs], 4096, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.360059  ,  0.6530824 ],\n",
       "       [ 4.589731  ,  0.7056192 ],\n",
       "       [ 4.066346  ,  0.61987317],\n",
       "       ...,\n",
       "       [ 6.9508705 ,  1.4646157 ],\n",
       "       [ 1.3789208 ,  1.3837237 ],\n",
       "       [-1.6033599 ,  1.7533748 ]], dtype=float32)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = np.array(preds)\n",
    "preds[:, :, 1] = np.abs(preds[:, :, 1]) # Make sure std is positive\n",
    "mean_preds = np.mean(preds, 0)\n",
    "\n",
    "repred = np.array(repred)\n",
    "repred[:, :, 1] = np.abs(repred[:, :, 1]) # Make sure std is positive\n",
    "mean_repred = np.mean(repred, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_info = data.iloc[:train_end,:2]\n",
    "test_info = data.iloc[eval_start:,:2]\n",
    "\n",
    "train_info = train_info.reset_index()\n",
    "test_info = test_info.reset_index()\n",
    "\n",
    "mean_preds_df = pd.DataFrame(mean_preds)\n",
    "mean_repred_df = pd.DataFrame(mean_repred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_combine = pd.concat([train_info, mean_repred_df], axis=1) \n",
    "test_combine = pd.concat([test_info, mean_preds_df], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_combine.to_csv('./nn_t2m_train.csv', index=False)\n",
    "test_combine.to_csv('./nn_t2m_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
